{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhKVdKLgCopwlG3spQ+Jhq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andy311p/Udemy_deep_learning_pytorch/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxgJo0p3yDCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True,\n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                            train=False,\n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfiHDftkyalx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = int(n_iters / (len(train_dataset)/batch_size))\n",
        "\n",
        "#iterable object\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKYWJNDRzEh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNModel,self).__init__()\n",
        "    \n",
        "    #Convolution 1\n",
        "    #use 16 kernels of depth 1 and size 5*5\n",
        "    self.cnn1 = nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5,stride=1,padding=2)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    #Max Pool 1\n",
        "    #downsample 28*28 -> 14*14\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    #Convolution 2\n",
        "    #use 32 kernels of depth 16 and size 5*5\n",
        "    self.cnn2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5,stride=1,padding=2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    #Max Pool 2\n",
        "    #downsample 14*14 -> 7*7\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "    #self.avgpool2 = nn.AvgPool2d(kernel_size=2)# can also be changed to Average pooling\n",
        "    \n",
        "    #Linear (Fully Connected)\n",
        "    self.fc1 = nn.Linear(32 * 7 * 7 ,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.cnn1(x)\n",
        "    out = self.relu1(out)\n",
        "    out = self.maxpool1(out)\n",
        "    out = self.cnn2(out)\n",
        "    out = self.relu2(out)\n",
        "    out = self.maxpool2(out)\n",
        "\n",
        "    #reshaping the out to fit a linear layer. technical step\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc1(out)\n",
        "    return out  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOzW6AWc0hOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#image size\n",
        "#input_dim = 28*28\n",
        "#hidden_dim = 100\n",
        "#num of classes(digits 0-9)\n",
        "#output_dim=10\n",
        "\n",
        "model = CNNModel()\n",
        "#computes the softmax automatically\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmOhleSr2-AB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9cec529c-f978-42ab-ddb2-c9a6d7870438"
      },
      "source": [
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images,labels) in enumerate(train_loader):\n",
        "    #images = Variable(images.view(-1,28*28))\n",
        "    images = Variable(images) #no need to reshape since CNN can take images directly\n",
        "    labels = Variable(labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    iter += 1\n",
        "    #every 500 iteration calculate accuracy\n",
        "    if iter % 500 == 0:\n",
        "      correct = 0.0\n",
        "      total = 0.0\n",
        "      #calc accuracy on test data\n",
        "      for images, labels in test_loader:\n",
        "        images = Variable(images)\n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predictions == labels).sum()\n",
        "\n",
        "      accuracy = 100 * correct / total\n",
        "      print('iteration: {}. loss: {} Accuracy: {}'.format(iter, loss.data,accuracy)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 500. loss: 0.24409785866737366 Accuracy: 93.33999633789062\n",
            "iteration: 1000. loss: 0.28489574790000916 Accuracy: 93.80000305175781\n",
            "iteration: 1500. loss: 0.0944928526878357 Accuracy: 95.75\n",
            "iteration: 2000. loss: 0.11147098243236542 Accuracy: 96.37000274658203\n",
            "iteration: 2500. loss: 0.09958186745643616 Accuracy: 96.73999786376953\n",
            "iteration: 3000. loss: 0.06651869416236877 Accuracy: 96.93000030517578\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}